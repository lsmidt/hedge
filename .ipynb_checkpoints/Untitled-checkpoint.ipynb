{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      " [[  450.    80. 14752.]\n",
      " [  300.    88. 11000.]\n",
      " [  260.    91.  9000.]\n",
      " [  496.    98.  1000.]\n",
      " [  200.    63.  2000.]]\n",
      "y_train:\n",
      " [[ 3.2]\n",
      " [ 1.8]\n",
      " [ 0.2]\n",
      " [ 1. ]\n",
      " [-1. ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b0478975e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHedgeLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#Lost and Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2b0478975e52>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import dbapi\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# AWS CREDENTIALS\n",
    "HOST = \"hedgedb.c288vca6ravj.us-east-2.rds.amazonaws.com\"\n",
    "PORT = 3306\n",
    "DB_NAME = \"scores_timeseries\"\n",
    "DB_USER = \"hedgeADMIN\"\n",
    "DB_PW = \"bluefootedboobie123\"\n",
    "\n",
    "# connect to Dataset and AWS to pull data \n",
    "scores_db = dataset.connect(\"sqlite:///scorebase.db\")\n",
    "AWS_RDS =  dataset.connect(\"mysql+pymysql://{}:{}@{}/{}\".format\\\n",
    "(DB_USER, DB_PW, HOST, DB_NAME), engine_kwargs = {'pool_recycle': 3600})\n",
    "\n",
    "\n",
    "in_size = 3 # twitter_sent, headline_sent, wiki_views\n",
    "out_size = 1 # composite output\n",
    "num_epochs = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "\n",
    "#Data set\n",
    "\n",
    "#x_train = np.array([[1.564],[2.11],[3.3],[5.4]], dtype=np.float32)\n",
    "x_train = np.array([[450.,80.,14752.],[300.,88.,11000.],[260.,91.,9000.],[496.,98.,1000.],[200.,63.,2000.]],dtype=np.float32)\n",
    "\n",
    "#y_train = np.array([[8.0],[19.0],[25.0],[34.45]], dtype= np.float32)\n",
    "y_train = np.array([[3.2],[1.8],[0.2],[1.0],[-1.0]],dtype=np.float32)\n",
    "\n",
    "print('x_train:\\n',x_train)\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "\n",
    "class HedgeLinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) #Forward propogation using linear model\n",
    "        return out\n",
    "\n",
    "model = HedgeLinearRegression(in_size,out_size)\n",
    "\n",
    "#Lost and Optimizer\n",
    "criterion = nn.MSELoss() # using Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) # using Stochastic Gradient Descent\n",
    "\n",
    "# train the Model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #convert numpy arrays for training and results to torch tensor Variable class\n",
    "    inputs = Variable(x_train)\n",
    "    target = Variable(y_train)\n",
    "\n",
    "    #forward, backward, optimize\n",
    "    outputs = model(inputs) # generate output from model with all input vectors\n",
    "    loss = criterion(outputs,targets) #loss function\n",
    "    \n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() #backward propogation\n",
    "    optimizer.step() #1-step optimization(gradient descent)\n",
    "    \n",
    "    if(epoch+1) % 1 ==0:\n",
    "        print('epoch [%d/%d], Loss: %.4f' % (epoch +1, num_epochs, loss.data[0]))\n",
    "        \n",
    "       \n",
    "\n",
    "model.eval()\n",
    "predicted = model(Variable(x_train)).data.numpy()\n",
    "      \n",
    "plt.plot(x_train.numpy(), y_train.numpy(),'ro',label='Original Data')\n",
    "plt.plot(x_train.numpy(), predicted,label='Fitted Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      " [[  450.    80. 14752.]\n",
      " [  300.    88. 11000.]\n",
      " [  260.    91.  9000.]\n",
      " [  496.    98.  1000.]\n",
      " [  200.    63.  2000.]]\n",
      "y_train:\n",
      " [[ 3.2]\n",
      " [ 1.8]\n",
      " [ 0.2]\n",
      " [ 1. ]\n",
      " [-1. ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-603c6da6ea84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#forward, backward, optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generate output from model with all input vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'targets' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import dbapi\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# AWS CREDENTIALS\n",
    "HOST = \"hedgedb.c288vca6ravj.us-east-2.rds.amazonaws.com\"\n",
    "PORT = 3306\n",
    "DB_NAME = \"scores_timeseries\"\n",
    "DB_USER = \"hedgeADMIN\"\n",
    "DB_PW = \"bluefootedboobie123\"\n",
    "\n",
    "# connect to Dataset and AWS to pull data \n",
    "scores_db = dataset.connect(\"sqlite:///scorebase.db\")\n",
    "AWS_RDS =  dataset.connect(\"mysql+pymysql://{}:{}@{}/{}\".format\\\n",
    "(DB_USER, DB_PW, HOST, DB_NAME), engine_kwargs = {'pool_recycle': 3600})\n",
    "\n",
    "\n",
    "in_size = 3 # twitter_sent, headline_sent, wiki_views\n",
    "out_size = 1 # composite output\n",
    "num_epochs = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "\n",
    "#Data set\n",
    "\n",
    "#x_train = np.array([[1.564],[2.11],[3.3],[5.4]], dtype=np.float32)\n",
    "x_train = np.array([[450.,80.,14752.],[300.,88.,11000.],[260.,91.,9000.],[496.,98.,1000.],[200.,63.,2000.]],dtype=np.float32)\n",
    "\n",
    "#y_train = np.array([[8.0],[19.0],[25.0],[34.45]], dtype= np.float32)\n",
    "y_train = np.array([[3.2],[1.8],[0.2],[1.0],[-1.0]],dtype=np.float32)\n",
    "\n",
    "print('x_train:\\n',x_train)\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) #Forward propogation using linear model\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(in_size,out_size)\n",
    "\n",
    "#Lost and Optimizer\n",
    "criterion = nn.MSELoss() # using Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) # using Stochastic Gradient Descent\n",
    "\n",
    "# train the Model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #convert numpy arrays for training and results to torch tensor Variable class\n",
    "    inputs = Variable(x_train)\n",
    "    target = Variable(y_train)\n",
    "\n",
    "    #forward, backward, optimize\n",
    "    outputs = model(inputs) # generate output from model with all input vectors\n",
    "    loss = criterion(outputs,targets) #loss function\n",
    "    \n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() #backward propogation\n",
    "    optimizer.step() #1-step optimization(gradient descent)\n",
    "    \n",
    "    if(epoch+1) % 1 ==0:\n",
    "        print('epoch [%d/%d], Loss: %.4f' % (epoch +1, num_epochs, loss.data[0]))\n",
    "        \n",
    "       \n",
    "\n",
    "model.eval()\n",
    "predicted = model(Variable(x_train)).data.numpy()\n",
    "      \n",
    "plt.plot(x_train.numpy(), y_train.numpy(),'ro',label='Original Data')\n",
    "plt.plot(x_train.numpy(), predicted,label='Fitted Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      " [[  450.    80. 14752.]\n",
      " [  300.    88. 11000.]\n",
      " [  260.    91.  9000.]\n",
      " [  496.    98.  1000.]\n",
      " [  200.    63.  2000.]]\n",
      "y_train:\n",
      " [[ 3.2]\n",
      " [ 1.8]\n",
      " [ 0.2]\n",
      " [ 1. ]\n",
      " [-1. ]]\n",
      "epoch [1/100], Loss: 507183.5312\n",
      "epoch [2/100], Loss: 5852543714244689920.0000\n",
      "epoch [3/100], Loss: 67672712767323803046348750585856.0000\n",
      "epoch [4/100], Loss: inf\n",
      "epoch [5/100], Loss: inf\n",
      "epoch [6/100], Loss: inf\n",
      "epoch [7/100], Loss: inf\n",
      "epoch [8/100], Loss: nan\n",
      "epoch [9/100], Loss: nan\n",
      "epoch [10/100], Loss: nan\n",
      "epoch [11/100], Loss: nan\n",
      "epoch [12/100], Loss: nan\n",
      "epoch [13/100], Loss: nan\n",
      "epoch [14/100], Loss: nan\n",
      "epoch [15/100], Loss: nan\n",
      "epoch [16/100], Loss: nan\n",
      "epoch [17/100], Loss: nan\n",
      "epoch [18/100], Loss: nan\n",
      "epoch [19/100], Loss: nan\n",
      "epoch [20/100], Loss: nan\n",
      "epoch [21/100], Loss: nan\n",
      "epoch [22/100], Loss: nan\n",
      "epoch [23/100], Loss: nan\n",
      "epoch [24/100], Loss: nan\n",
      "epoch [25/100], Loss: nan\n",
      "epoch [26/100], Loss: nan\n",
      "epoch [27/100], Loss: nan\n",
      "epoch [28/100], Loss: nan\n",
      "epoch [29/100], Loss: nan\n",
      "epoch [30/100], Loss: nan\n",
      "epoch [31/100], Loss: nan\n",
      "epoch [32/100], Loss: nan\n",
      "epoch [33/100], Loss: nan\n",
      "epoch [34/100], Loss: nan\n",
      "epoch [35/100], Loss: nan\n",
      "epoch [36/100], Loss: nan\n",
      "epoch [37/100], Loss: nan\n",
      "epoch [38/100], Loss: nan\n",
      "epoch [39/100], Loss: nan\n",
      "epoch [40/100], Loss: nan\n",
      "epoch [41/100], Loss: nan\n",
      "epoch [42/100], Loss: nan\n",
      "epoch [43/100], Loss: nan\n",
      "epoch [44/100], Loss: nan\n",
      "epoch [45/100], Loss: nan\n",
      "epoch [46/100], Loss: nan\n",
      "epoch [47/100], Loss: nan\n",
      "epoch [48/100], Loss: nan\n",
      "epoch [49/100], Loss: nan\n",
      "epoch [50/100], Loss: nan\n",
      "epoch [51/100], Loss: nan\n",
      "epoch [52/100], Loss: nan\n",
      "epoch [53/100], Loss: nan\n",
      "epoch [54/100], Loss: nan\n",
      "epoch [55/100], Loss: nan\n",
      "epoch [56/100], Loss: nan\n",
      "epoch [57/100], Loss: nan\n",
      "epoch [58/100], Loss: nan\n",
      "epoch [59/100], Loss: nan\n",
      "epoch [60/100], Loss: nan\n",
      "epoch [61/100], Loss: nan\n",
      "epoch [62/100], Loss: nan\n",
      "epoch [63/100], Loss: nan\n",
      "epoch [64/100], Loss: nan\n",
      "epoch [65/100], Loss: nan\n",
      "epoch [66/100], Loss: nan\n",
      "epoch [67/100], Loss: nan\n",
      "epoch [68/100], Loss: nan\n",
      "epoch [69/100], Loss: nan\n",
      "epoch [70/100], Loss: nan\n",
      "epoch [71/100], Loss: nan\n",
      "epoch [72/100], Loss: nan\n",
      "epoch [73/100], Loss: nan\n",
      "epoch [74/100], Loss: nan\n",
      "epoch [75/100], Loss: nan\n",
      "epoch [76/100], Loss: nan\n",
      "epoch [77/100], Loss: nan\n",
      "epoch [78/100], Loss: nan\n",
      "epoch [79/100], Loss: nan\n",
      "epoch [80/100], Loss: nan\n",
      "epoch [81/100], Loss: nan\n",
      "epoch [82/100], Loss: nan\n",
      "epoch [83/100], Loss: nan\n",
      "epoch [84/100], Loss: nan\n",
      "epoch [85/100], Loss: nan\n",
      "epoch [86/100], Loss: nan\n",
      "epoch [87/100], Loss: nan\n",
      "epoch [88/100], Loss: nan\n",
      "epoch [89/100], Loss: nan\n",
      "epoch [90/100], Loss: nan\n",
      "epoch [91/100], Loss: nan\n",
      "epoch [92/100], Loss: nan\n",
      "epoch [93/100], Loss: nan\n",
      "epoch [94/100], Loss: nan\n",
      "epoch [95/100], Loss: nan\n",
      "epoch [96/100], Loss: nan\n",
      "epoch [97/100], Loss: nan\n",
      "epoch [98/100], Loss: nan\n",
      "epoch [99/100], Loss: nan\n",
      "epoch [100/100], Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import dbapi\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# AWS CREDENTIALS\n",
    "HOST = \"hedgedb.c288vca6ravj.us-east-2.rds.amazonaws.com\"\n",
    "PORT = 3306\n",
    "DB_NAME = \"scores_timeseries\"\n",
    "DB_USER = \"hedgeADMIN\"\n",
    "DB_PW = \"bluefootedboobie123\"\n",
    "\n",
    "# connect to Dataset and AWS to pull data \n",
    "scores_db = dataset.connect(\"sqlite:///scorebase.db\")\n",
    "AWS_RDS =  dataset.connect(\"mysql+pymysql://{}:{}@{}/{}\".format\\\n",
    "(DB_USER, DB_PW, HOST, DB_NAME), engine_kwargs = {'pool_recycle': 3600})\n",
    "\n",
    "\n",
    "in_size = 3 # twitter_sent, headline_sent, wiki_views\n",
    "out_size = 1 # composite output\n",
    "num_epochs = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "\n",
    "#Data set\n",
    "\n",
    "#x_train = np.array([[1.564],[2.11],[3.3],[5.4]], dtype=np.float32)\n",
    "x_train = np.array([[450.,80.,14752.],[300.,88.,11000.],[260.,91.,9000.],[496.,98.,1000.],[200.,63.,2000.]],dtype=np.float32)\n",
    "\n",
    "#y_train = np.array([[8.0],[19.0],[25.0],[34.45]], dtype= np.float32)\n",
    "y_train = np.array([[3.2],[1.8],[0.2],[1.0],[-1.0]],dtype=np.float32)\n",
    "\n",
    "print('x_train:\\n',x_train)\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) #Forward propogation using linear model\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(in_size,out_size)\n",
    "\n",
    "#Lost and Optimizer\n",
    "criterion = nn.MSELoss() # using Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) # using Stochastic Gradient Descent\n",
    "\n",
    "# train the Model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #convert numpy arrays for training and results to torch tensor Variable class\n",
    "    inputs = Variable(x_train)\n",
    "    target = Variable(y_train)\n",
    "\n",
    "    #forward, backward, optimize\n",
    "    outputs = model(inputs) # generate output from model with all input vectors\n",
    "    loss = criterion(outputs,target) #loss function\n",
    "    \n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() #backward propogation\n",
    "    optimizer.step() #1-step optimization(gradient descent)\n",
    "    \n",
    "    if(epoch+1) % 1 ==0:\n",
    "        print('epoch [%d/%d], Loss: %.4f' % (epoch +1, num_epochs, loss.data[0]))\n",
    "        \n",
    "       \n",
    "\n",
    "model.eval()\n",
    "predicted = model(Variable(x_train)).data.numpy()\n",
    "      \n",
    "plt.plot(x_train.numpy(), y_train.numpy(),'ro',label='Original Data')\n",
    "plt.plot(x_train.numpy(), predicted,label='Fitted Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      " [[  450.    80. 14752.]\n",
      " [  300.    88. 11000.]\n",
      " [  260.    91.  9000.]\n",
      " [  496.    98.  1000.]\n",
      " [  200.    63.  2000.]]\n",
      "y_train:\n",
      " [[3.2]\n",
      " [1.8]\n",
      " [0.2]\n",
      " [1. ]\n",
      " [0.5]]\n",
      "epoch [1/100], Loss: 4801817.5000\n",
      "epoch [2/100], Loss: 55299251223738187776.0000\n",
      "epoch [3/100], Loss: 639422878115500913670109536452608.0000\n",
      "epoch [4/100], Loss: inf\n",
      "epoch [5/100], Loss: inf\n",
      "epoch [6/100], Loss: inf\n",
      "epoch [7/100], Loss: inf\n",
      "epoch [8/100], Loss: nan\n",
      "epoch [9/100], Loss: nan\n",
      "epoch [10/100], Loss: nan\n",
      "epoch [11/100], Loss: nan\n",
      "epoch [12/100], Loss: nan\n",
      "epoch [13/100], Loss: nan\n",
      "epoch [14/100], Loss: nan\n",
      "epoch [15/100], Loss: nan\n",
      "epoch [16/100], Loss: nan\n",
      "epoch [17/100], Loss: nan\n",
      "epoch [18/100], Loss: nan\n",
      "epoch [19/100], Loss: nan\n",
      "epoch [20/100], Loss: nan\n",
      "epoch [21/100], Loss: nan\n",
      "epoch [22/100], Loss: nan\n",
      "epoch [23/100], Loss: nan\n",
      "epoch [24/100], Loss: nan\n",
      "epoch [25/100], Loss: nan\n",
      "epoch [26/100], Loss: nan\n",
      "epoch [27/100], Loss: nan\n",
      "epoch [28/100], Loss: nan\n",
      "epoch [29/100], Loss: nan\n",
      "epoch [30/100], Loss: nan\n",
      "epoch [31/100], Loss: nan\n",
      "epoch [32/100], Loss: nan\n",
      "epoch [33/100], Loss: nan\n",
      "epoch [34/100], Loss: nan\n",
      "epoch [35/100], Loss: nan\n",
      "epoch [36/100], Loss: nan\n",
      "epoch [37/100], Loss: nan\n",
      "epoch [38/100], Loss: nan\n",
      "epoch [39/100], Loss: nan\n",
      "epoch [40/100], Loss: nan\n",
      "epoch [41/100], Loss: nan\n",
      "epoch [42/100], Loss: nan\n",
      "epoch [43/100], Loss: nan\n",
      "epoch [44/100], Loss: nan\n",
      "epoch [45/100], Loss: nan\n",
      "epoch [46/100], Loss: nan\n",
      "epoch [47/100], Loss: nan\n",
      "epoch [48/100], Loss: nan\n",
      "epoch [49/100], Loss: nan\n",
      "epoch [50/100], Loss: nan\n",
      "epoch [51/100], Loss: nan\n",
      "epoch [52/100], Loss: nan\n",
      "epoch [53/100], Loss: nan\n",
      "epoch [54/100], Loss: nan\n",
      "epoch [55/100], Loss: nan\n",
      "epoch [56/100], Loss: nan\n",
      "epoch [57/100], Loss: nan\n",
      "epoch [58/100], Loss: nan\n",
      "epoch [59/100], Loss: nan\n",
      "epoch [60/100], Loss: nan\n",
      "epoch [61/100], Loss: nan\n",
      "epoch [62/100], Loss: nan\n",
      "epoch [63/100], Loss: nan\n",
      "epoch [64/100], Loss: nan\n",
      "epoch [65/100], Loss: nan\n",
      "epoch [66/100], Loss: nan\n",
      "epoch [67/100], Loss: nan\n",
      "epoch [68/100], Loss: nan\n",
      "epoch [69/100], Loss: nan\n",
      "epoch [70/100], Loss: nan\n",
      "epoch [71/100], Loss: nan\n",
      "epoch [72/100], Loss: nan\n",
      "epoch [73/100], Loss: nan\n",
      "epoch [74/100], Loss: nan\n",
      "epoch [75/100], Loss: nan\n",
      "epoch [76/100], Loss: nan\n",
      "epoch [77/100], Loss: nan\n",
      "epoch [78/100], Loss: nan\n",
      "epoch [79/100], Loss: nan\n",
      "epoch [80/100], Loss: nan\n",
      "epoch [81/100], Loss: nan\n",
      "epoch [82/100], Loss: nan\n",
      "epoch [83/100], Loss: nan\n",
      "epoch [84/100], Loss: nan\n",
      "epoch [85/100], Loss: nan\n",
      "epoch [86/100], Loss: nan\n",
      "epoch [87/100], Loss: nan\n",
      "epoch [88/100], Loss: nan\n",
      "epoch [89/100], Loss: nan\n",
      "epoch [90/100], Loss: nan\n",
      "epoch [91/100], Loss: nan\n",
      "epoch [92/100], Loss: nan\n",
      "epoch [93/100], Loss: nan\n",
      "epoch [94/100], Loss: nan\n",
      "epoch [95/100], Loss: nan\n",
      "epoch [96/100], Loss: nan\n",
      "epoch [97/100], Loss: nan\n",
      "epoch [98/100], Loss: nan\n",
      "epoch [99/100], Loss: nan\n",
      "epoch [100/100], Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAG0pJREFUeJzt3X1wVfW97/H3V0RiBEUh0yIRdlrxSACJEETL8Q6VCuhFbKmtOEwtnqOpYK+0x3q1halVh85RO/VYnzAtXuvprlahWI9Dp7YH8WCdiwTLsyKgAYOeGvH6QAMW9Hv/WItkJ+xk7yRrZ++98nnN7Ml6+GWtb9bO+mRlrbV/y9wdERGJl2PyXYCIiERP4S4iEkMKdxGRGFK4i4jEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURi6Nh8rXjw4MGeSCTytXoRkaK0fv36d929LFO7vIV7IpGgrq4uX6sXESlKZrY7m3Y6LSMiEkMKdxGRGFK4i4jEUN7OuUu8HDp0iIaGBg4ePJjvUiRFSUkJ5eXl9O3bN9+lSA9TuEskGhoaGDBgAIlEAjPLdzkCuDv79u2joaGBioqKfJcjPaz4Tsskk5BIwDHHBF+TyezmSU4dPHiQQYMGKdgLiJkxaNAg/TdVSHowo4rryD2ZhJoaaGoKxnfvDsaPaG/enDk9W2cvpWAvPHpPCkhH+ZWDjLJ8PWavurraO32feyIRbJC2hg8PvrY3r76+s+VJJ73yyiuMHDky32VIGnpvCkRH+dWJjDKz9e5enaldcZ2W2bOn/ekdzZNeoaGhgUsvvZQRI0bw+c9/ngULFvD3v/89bdu33nqLyy67LOMyL774Yt5///0u1fOjH/2In/zkJ2mnDx06lKqqKkaMGMGsWbPYtm1bxuU98sgjvPXWW12qRQpAD2dUcYX7sGHtT+9onhSeiM89ujuzZs3iy1/+Mjt27OC1115j//79LFy48Ki2hw8f5tRTT2XZsmUZl7ty5UoGDhzYrdrS+e53v8uGDRvYsWMHl19+ORdccAGNjY0dfo/Cvcj1cEYVV7gvXgylpa2nlZYG0zuaJ4XlyLnH3bvBveXcYzcCftWqVZSUlHDVVVcB0KdPH+6++24efvhhmpqaeOSRR5g5cyYXXHABU6ZMob6+ntGjRwPQ1NTE17/+dSorK/nKV77CxIkTm7vGSCQSvPvuu9TX1zNy5EiuueYaRo0axdSpUzlw4AAAP//5z5kwYQJjx47lq1/9Kk1Hzqlm6fLLL2fq1Kn8+te/BuC2225jwoQJjB49mpqaGtydZcuWUVdXx5w5c6iqquLAgQNp20kB6+GMKq5wnzMHamuDc1Rmwdfa2mB6R/OksCxc2HJR6YimpmB6F23dupXx48e3mnbiiScybNgwdu7cCcDLL7/MsmXLeP7551u1e+CBBzj55JPZtm0bt99+O+vXr0+7jh07dnDdddexdetWBg4cyPLlywGYNWsW69atY+PGjYwcOZKlS5d2uv5x48bx6quvAvDtb3+bdevWsWXLFg4cOMAzzzzDZZddRnV1Nclkkg0bNnD88cenbScFrIczqrjuloGWIO/sPCkcebo+cuGFF3LKKaccNf2FF15gwYIFAIwePZqzzjor7fdXVFRQVVUFwPjx46kPL4Jt2bKFRYsW8f7777N//36mTZvW6dpSj7qfe+457rzzTpqamnjvvfcYNWoUl1xyyVHfk207KSA9mFHFdeQu8ZCDc4+VlZVHHXF/+OGH7Nmzh9NPPx2AE044ocvLB+jXr1/zcJ8+fTh8+DAAc+fO5b777mPz5s3ccsstXbqv/C9/+QsjR47k4MGDzJ8/n2XLlrF582auueaatMvLtp30Xgp36Xk5OPc4ZcoUmpqaePTRRwH45JNPuOGGG5g7dy6lbdfVxqRJk3jiiScA2LZtG5s3b+7Uuj/66COGDBnCoUOHSHbhusHy5ct59tlnueKKK5oDevDgwezfv7/VRd8BAwbw0UcfAXTYTgQU7pIPOTj3aGasWLGCJ598khEjRnDGGWdQUlLCj3/844zfO3/+fBobG6msrGTRokWMGjWKk046Ket133777UycOJFJkyZx5plnZvU9d999d/OtkL/61a9YtWoVZWVlDBw4kGuuuYbRo0czbdo0JkyY0Pw9c+fO5dprr6Wqqop+/fq1204Eiu1DTFKwivmDMp988gmHDh2ipKSEXbt28aUvfYnt27dz3HHH5bu0SBTzeyNHy/ZDTMV3QVUkYk1NTXzxi1/k0KFDuDsPPPBAbIJdeq+M4W5mJcB/Af3C9svc/ZY2bfoBjwLjgX3A5e5eH3m1IjkwYMAAPfJRYiebc+4fAxe4+1igCphuZue2afPPwP9z99OBu4E7oi1TREQ6I2O4e2B/ONo3fLU9UX8p8MtweBkwxdQdnYhI3mR1t4yZ9TGzDcA7wB/dfW2bJkOBNwHc/TDwATAozXJqzKzOzOoy9aMhIiJdl1W4u/sn7l4FlAPnmNnorqzM3Wvdvdrdq8vKyrqyCBERyUKn7nN39/eB54DpbWbtBU4DMLNjgZMILqyK9Bh1+SvSImO4m1mZmQ0Mh48HLgRebdPsaeCb4fBlwCpXF3XSEXX5qy5/JaeyOXIfAjxnZpuAdQTn3J8xs9vMbGbYZikwyMx2Av8C3JybciUW1OVvK+ryV3Ihm7tlNrn72e5+lruPdvfbwuk/dPenw+GD7v41dz/d3c9x99dzXbgUMXX5exR1+StRU98y0vMKsMvf2bNnA13v8vf8889nzJgxJJNJtm7d2una2nb5O3HiRMaMGcOqVavaXV627aR3UrhLz1OXv0dRl78SNYW79Dx1+duKuvyVXFC4S89Tl7/q8ldyTl3+SiSKuVtZdfkrxURd/opkSV3+Shwp3KXXU5e/Ekc65y4iEkMKdxGRGFK4i4jEkMJdRCSGFO4SG+ryV6SFwl3yQ13+qstfySmFu/Q8dfnbirr8lVxQuEvPU5e/R1GXvxI1hbv0PHX5exR1+StRU7hLz1OXv0dRl78SNYW79Dx1+duKuvyVXFC4S89Tl7/q8ldyTl3+SiSKuVtZdfkrxURd/opkSV3+Shwp3KXXU5e/Ekc65y4iEkMKdxGRGMoY7mZ2mpk9Z2bbzGyrmS1I02aymX1gZhvC1w9zU66IiGQjm3Puh4Eb3P1lMxsArDezP7p7227s1rj7jOhLFBGRzsp45O7ub7v7y+HwR8ArwNBcFybSWX369KGqqqr5VV9fT11dHddffz0Aq1ev5sUXX2xu/9RTT2XV1W5b/fv3z3r6kiVLmj9YJdKTOnW3jJklgLOBtWlmn2dmG4G3gO+5+1EdXZhZDVADMKwbHzUXSef4449nw4YNraYlEgmqq4NbglevXk3//v35whe+AAThPmPGDCorK3NW07XXXpuzZYt0JOsLqmbWH1gOfMfdP2wz+2VguLuPBe4Fnkq3DHevdfdqd68uKyvras0iWVu9ejUzZsygvr6eJUuWNH8y9Pnnn+fpp5/mxhtvpKqqil27drFr1y6mT5/O+PHjOf/885t7aXzjjTc477zzGDNmDIsWLerU+lMf2DF58mRuuukmzjnnHM444wzWrFkDBB+iuvHGG5kwYQJnnXUWDz30ULQbQXqlrI7czawvQbAn3f23beenhr27rzSzB8xssLu/G12pUixu/Y+tbHur7d//7qk89URuuWRUh20OHDjQ3GtjRUUFK1asaJ6XSCS49tpr6d+/P9/73vcAmDlzJjNmzGh+ItOUKVNYsmQJI0aMYO3atcyfP59Vq1axYMEC5s2bx5VXXsn999/frZ/j8OHDvPTSS6xcuZJbb72VP/3pTyxdupSTTjqJdevW8fHHHzNp0iSmTp1KRUVFt9YlvVvGcDczA5YCr7j7T9tp81ngr+7uZnYOwX8E+yKtVCSDdKdlsrV//35efPFFvva1rzVP+/jjjwH485//3Nx3+ze+8Q1uuummLtc4a9YsoHWXwc8++yybNm1q7vzrgw8+YMeOHQp36ZZsjtwnAd8ANpvZkT3nB8AwAHdfAlwGzDOzw8ABYLbrsTC9VqYj7EL06aefMnDgwHb/OATHON13pNvg1C6D3Z17772XadOmRbIOEcjubpkX3N3c/Sx3rwpfK919SRjsuPt97j7K3ce6+7nu/mKm5Yr0tNQuc9uOn3jiiVRUVPDkk08CQeBu3LgRCLoEfvzxxwG61KVvJtOmTePBBx/k0KFDALz22mv87W9/i3w90rvoE6rSa1xyySWsWLGCqqoq1qxZw+zZs7nrrrs4++yz2bVrF8lkkqVLlzJ27FhGjRrF7373OwDuuece7r//fsaMGcPevXvbXX5TUxPl5eXNr5/+NO1ZzKNcffXVVFZWMm7cOEaPHs23vvWt5qN6ka5Sl78SCXUrW7j03sRLtl3+6shdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCneJDXX5K9JCD8iW2FCXvyItdOQusaYuf6W30pG7RO/3N8N/b452mZ8dAxf9a4dN1OWvSAuFu8SGuvwVaaFwl+hlOMIuROryV+JG59yl11CXv9KbKNyl11CXv9KbqMtfiYS6lS1cem/iRV3+ioj0Ygp3EZEYUriLiMSQwl1EJIYU7iIiMaRwFxGJIYW7xIa6/BVpkbH7ATM7DXgU+AzgQK2739OmjQH3ABcDTcBcd385+nJF2qcuf0VaZHPkfhi4wd0rgXOB68ys7d5wETAifNUAD0ZapUgXqctf6a0yHrm7+9vA2+HwR2b2CjAUSP1/9lLgUQ8+7vp/zWygmQ0Jv1d6mTteuoNX33s10mWeecqZ3HROx70xqstfkRad6hXSzBLA2cDaNrOGAm+mjDeE01qFu5nVEBzZM2zYsM5VKpKBuvwVaZF1uJtZf2A58B13/7ArK3P3WqAWgr5lurIMKXyZjrALkbr8lbjJ6m4ZM+tLEOxJd/9tmiZ7gdNSxsvDaSIFQ13+Sm+SMdzDO2GWAq+4e3t9mD4NXGmBc4EPdL5dCo26/JXeJGOXv2b2j8AaYDPwaTj5B8AwAHdfEv4BuA+YTnAr5FXu3mF/vuryN17UrWzh0nsTL9l2+ZvN3TIvAB2ecAzvkrku+/JERCSX9AlVEZEYUrhLZPL1VC9pn96T3kvhLpEoKSlh3759CpMC4u7s27ePkpKSfJciedCpDzGJtKe8vJyGhgYaGxvzXYqkKCkpoby8PN9lSB4UX7gnk7BwIezZA8OGwcUXw8qVLeOLF8OcOfmustfp27evPlHZm7TdD7XfFZziCvdkEmpqoKkpGN+9Gx5M6aNs9+5gPugXTSRX0u2H2u8KTsb73HOlS/e5JxLBL1Imw4dD2G+HiESsvf1Q+12PyPY+9+K6oLpnT7TtRKTz2tu/tN8VlOIK92x7klSPkyK5097+pf2uoBRXuC9eDKWlHbcpLQ3aiUhupNsPtd8VnOIK9zlzoLY2OLdnFnydN6/1eG2tLuqI5FK6/VD7XcEprguqIiK9XDwvqIqISFYU7iIiMaRwFxGJIYW7iEgMKdxFRGJI4S4iEkMKdxGRGFK4i4jEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDGUMdzN72MzeMbMt7cyfbGYfmNmG8PXD6MsUEZHOyOYZqo8A9wGPdtBmjbvPiKQiERHptoxH7u7+X8B7PVCLiIhEJKpz7ueZ2UYz+72ZjWqvkZnVmFmdmdU1NjZGtGoREWkrinB/GRju7mOBe4Gn2mvo7rXuXu3u1WVlZRGsWkRE0ul2uLv7h+6+PxxeCfQ1s8HdrkxERLqs2+FuZp81MwuHzwmXua+7yxURka7LeLeMmT0GTAYGm1kDcAvQF8DdlwCXAfPM7DBwAJjt+Xowq4iIAFmEu7tfkWH+fQS3SoqISIHQJ1RFRGJI4S4iEkMKdxGRGFK4i4jEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuIxJDCXUQkhhTuIiIxpHAXEYkhhbuISAwp3EVEYkjhLiISQwp3EZEYUriLiMSQwl1EJIYU7iIiMaRwFxGJIYW7iEgMZQx3M3vYzN4xsy3tzDcz+5mZ7TSzTWY2LvoyUySTkEjAMccEX5PJzs3valsRkSKSzZH7I8D0DuZfBIwIXzXAg90vqx3JJNTUwO7d4B58ralpCeVM8zuzLBGRImbunrmRWQJ4xt1Hp5n3ELDa3R8Lx7cDk9397Y6WWV1d7XV1dZ2rNpEIQrit4cOhvj7z/M4sS0SkAJnZenevztQuinPuQ4E3U8Ybwmnpiqoxszozq2tsbOz8mvbs6Xh6pvmdWZaISBHr0Quq7l7r7tXuXl1WVtb5BQwb1vH0TPM7sywRkSIWRbjvBU5LGS8Pp0Vv8WIoLW09rbQ0mJ7N/M4sS0SkiEUR7k8DV4Z3zZwLfJDpfHuXzZkDtbXBeXGz4GttbTA9m/mdWZaISBHLeEHVzB4DJgODgb8CtwB9Adx9iZkZcB/BHTVNwFXunvFKaZcuqIqI9HLZXlA9NlMDd78iw3wHrutEbSIikmP6hKqISAwp3EVEYkjhLiISQwp3EZEYUriLiMSQwl1EJIYU7iIiMaRwFxGJIYW7iEgMKdxFRGJI4S4iEkMKdxGRGFK4i4jEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuIxJDCXUQkhhTuIiIxlFW4m9l0M9tuZjvN7OY08+eaWaOZbQhfV0dfqoiIZCtjuJtZH+B+4CKgErjCzCrTNP2Nu1eFr19EXGcgmYREAo45BgYPDl7HHBNMSyZzsspOS62xkOoSkV7l2CzanAPsdPfXAczsceBSYFsuCztKMgk1NdDUFIzv29cyb/fuYB7AnDk9WlYrbWsslLpEpNfJ5rTMUODNlPGGcFpbXzWzTWa2zMxOi6S6VAsXtoRmOk1NQZt8SldjIdQlIr1OVBdU/wNIuPtZwB+BX6ZrZGY1ZlZnZnWNjY2dW8OePdG0yaX21p/vukSk18km3PcCqUfi5eG0Zu6+z90/Dkd/AYxPtyB3r3X3anevLisr61ylw4ZF0yaX2lt/vusSkV4nm3BfB4wwswozOw6YDTyd2sDMhqSMzgReia7E0OLFUFra/vzS0qBNPqWrsRDqEpFeJ2O4u/th4NvAHwhC+wl332pmt5nZzLDZ9Wa21cw2AtcDcyOvdM4cqK2F4cPBDAYNCl5mwbTa2vxftGxbY6HUJSK9jrl7XlZcXV3tdXV1eVm3iEixMrP17l6dqZ0+oSoiEkMKdxGRGFK4i4jEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuIxJDCXUQkhoov3Ns+o3T+fD2zVOJFz+GVCGTzDNXCke4ZpQ8+2DJfzyyVYqfn8EpEiqvL30Qi+GXPZPhwqK/vSlki+dXe77h+pyUUzy5/s30WqZ5ZKsVKz+GViBRXuGf7LFI9s1SKlZ7DKxEprnDP9BxV0DNLpbjpObwSkeIK93TPKJ03T88slfjQc3glIsV1QVVEpJeL5wVVERHJisJdRCSGFO4iIjGkcBcRiSGFu4hIDOXtbhkzawSy6EvgKIOBdyMuJxeKoU7VGJ1iqLMYaoTiqDOfNQ5397JMjfIW7l1lZnXZ3AaUb8VQp2qMTjHUWQw1QnHUWQw16rSMiEgMKdxFRGKoGMO9Nt8FZKkY6lSN0SmGOouhRiiOOgu+xqI75y4iIpkV45G7iIhkUFThbmbTzWy7me00s5t7eN2nmdlzZrbNzLaa2YJw+ilm9kcz2xF+PTmcbmb2s7DWTWY2LmVZ3wzb7zCzb+ag1j5m9hczeyYcrzCztWEtvzGz48Lp/cLxneH8RMoyvh9O325m03JQ40AzW2Zmr5rZK2Z2XqFtSzP7bvhebzGzx8yspBC2pZk9bGbvmNmWlGmRbTszG29mm8Pv+ZmZWUQ13hW+35vMbIWZDUyZl3YbtbfPt/c+RFFnyrwbzMzNbHA4npdt2WXuXhQvoA+wC/gccBywEajswfUPAcaFwwOA14BK4E7g5nD6zcAd4fDFwO8BA84F1obTTwFeD7+eHA6fHHGt/wL8GngmHH8CmB0OLwHmhcPzgSXh8GzgN+FwZbh9+wEV4XbvE3GNvwSuDoePAwYW0rYEhgJvAMenbMO5hbAtgf8BjAO2pEyLbNsBL4VtLfzeiyKqcSpwbDh8R0qNabcRHezz7b0PUdQZTj8N+APBZ3EG53Nbdvn3pKdW1O1C4TzgDynj3we+n8d6fgdcCGwHhoTThgDbw+GHgCtS2m8P518BPJQyvVW7COoqB/4TuAB4Jvylejdlp2rejuEv73nh8LFhO2u7bVPbRVTjSQTBaW2mF8y2JAj3N8Md9thwW04rlG0JJGgdnJFsu3DeqynTW7XrTo1t5n0FSIbDabcR7ezzHf1OR1UnsAwYC9TTEu5525ZdeRXTaZkjO9sRDeG0Hhf+y302sBb4jLu/Hc76b+Az4XB79eb65/g34H8Dn4bjg4D33f1wmvU11xLO/yBsn+saK4BG4P9YcProF2Z2AgW0Ld19L/ATYA/wNsG2WU/hbcsjotp2Q8PhXNf7TwRHsl2psaPf6W4zs0uBve6+sc2sQt2WaRVTuBcEM+sPLAe+4+4fps7z4M9z3m4/MrMZwDvuvj5fNWTpWIJ/hR9097OBvxGcSmhWANvyZOBSgj9EpwInANPzVU9n5HvbZWJmC4HDQDLftbRlZqXAD4Af5ruW7iqmcN9LcB7siPJwWo8xs74EwZ5099+Gk/9qZkPC+UOAd8Lp7dWby59jEjDTzOqBxwlOzdwDDDSzY9Osr7mWcP5JwL4c1wjBEUyDu68Nx5cRhH0hbcsvAW+4e6O7HwJ+S7B9C21bHhHVttsbDuekXjObC8wA5oR/hLpS4z7afx+66/MEf9A3hvtROfCymX22C3XmdFtm1FPnf7r7Ijjae51gwx+5uDKqB9dvwKPAv7WZfhetL2TdGQ7/T1pffHkpnH4Kwfnmk8PXG8ApOah3Mi0XVJ+k9cWn+eHwdbS+CPhEODyK1he4Xif6C6prgH8Ih38UbseC2ZbARGArUBqu95fA/yqUbcnR59wj23YcfRHw4ohqnA5sA8ratEu7jehgn2/vfYiizjbz6mk55563bdmln6unVhRJscHV6tcIrqAv7OF1/yPBv7qbgA3h62KC83//CewA/pTyphpwf1jrZqA6ZVn/BOwMX1flqN7JtIT758Jfsp3hTtEvnF4Sju8M538u5fsXhrVvJwdX+IEqoC7cnk+FO0VBbUvgVuBVYAvw72H45H1bAo8RXAc4RPBf0D9Hue2A6vBn3gXcR5sL392ocSfBuekj+8+STNuIdvb59t6HKOpsM7+elnDPy7bs6kufUBURiaFiOucuIiJZUriLiMSQwl1EJIYU7iIiMaRwFxGJIYW7iEgMKdxFRGJI4S4iEkP/H4Ip/zhKNtIsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import dbapi\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# AWS CREDENTIALS\n",
    "HOST = \"hedgedb.c288vca6ravj.us-east-2.rds.amazonaws.com\"\n",
    "PORT = 3306\n",
    "DB_NAME = \"scores_timeseries\"\n",
    "DB_USER = \"hedgeADMIN\"\n",
    "DB_PW = \"bluefootedboobie123\"\n",
    "\n",
    "# connect to Dataset and AWS to pull data \n",
    "scores_db = dataset.connect(\"sqlite:///scorebase.db\")\n",
    "AWS_RDS =  dataset.connect(\"mysql+pymysql://{}:{}@{}/{}\".format\\\n",
    "(DB_USER, DB_PW, HOST, DB_NAME), engine_kwargs = {'pool_recycle': 3600})\n",
    "\n",
    "\n",
    "in_size = 3 # twitter_sent, headline_sent, wiki_views\n",
    "out_size = 1 # composite output\n",
    "num_epochs = 100\n",
    "learning_rate = 0.02\n",
    "\n",
    "\n",
    "#Data set\n",
    "\n",
    "#x_train = np.array([[1.564],[2.11],[3.3],[5.4]], dtype=np.float32)\n",
    "x_train = np.array([[450.,80.,14752.],[300.,88.,11000.],[260.,91.,9000.],[496.,98.,1000.],[200.,63.,2000.]],dtype=np.float32)\n",
    "\n",
    "#y_train = np.array([[8.0],[19.0],[25.0],[34.45]], dtype= np.float32)\n",
    "y_train = np.array([[3.2],[1.8],[0.2],[1.0],[0.5]],dtype=np.float32)\n",
    "\n",
    "print('x_train:\\n',x_train)\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) #Forward propogation using linear model\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(in_size,out_size)\n",
    "\n",
    "#Lost and Optimizer\n",
    "criterion = nn.MSELoss() # using Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) # using Stochastic Gradient Descent\n",
    "\n",
    "# train the Model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #convert numpy arrays for training and results to torch tensor Variable class\n",
    "    inputs = Variable(x_train)\n",
    "    target = Variable(y_train)\n",
    "\n",
    "    #forward, backward, optimize\n",
    "    outputs = model(inputs) # generate output from model with all input vectors\n",
    "    loss = criterion(outputs,target) #loss function\n",
    "    \n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() #backward propogation\n",
    "    optimizer.step() #1-step optimization(gradient descent)\n",
    "    \n",
    "    if(epoch+1) % 1 ==0:\n",
    "        print('epoch [%d/%d], Loss: %.4f' % (epoch +1, num_epochs, loss.data[0]))\n",
    "        \n",
    "       \n",
    "\n",
    "model.eval()\n",
    "predicted = model(Variable(x_train)).data.numpy()\n",
    "      \n",
    "plt.plot(x_train.numpy(), y_train.numpy(),'ro',label='Original Data')\n",
    "plt.plot(x_train.numpy(), predicted,label='Fitted Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      " [[450. 400. 400.]\n",
      " [300. 300. 350.]\n",
      " [340. 333. 400.]\n",
      " [421. 308. 309.]\n",
      " [350. 440. 400.]]\n",
      "y_train:\n",
      " [[320.]\n",
      " [180.]\n",
      " [ 20.]\n",
      " [100.]\n",
      " [ 50.]]\n",
      "epoch [1/100], Loss: 53472.3125\n",
      "epoch [2/100], Loss: 1033406528.0000\n",
      "epoch [3/100], Loss: 27114445209600.0000\n",
      "epoch [4/100], Loss: 711433226316414976.0000\n",
      "epoch [5/100], Loss: 18666698753610419798016.0000\n",
      "epoch [6/100], Loss: 489779870835943779147448320.0000\n",
      "epoch [7/100], Loss: 12850924983833014253776940302336.0000\n",
      "epoch [8/100], Loss: 337184641157338395037729501600546816.0000\n",
      "epoch [9/100], Loss: inf\n",
      "epoch [10/100], Loss: inf\n",
      "epoch [11/100], Loss: inf\n",
      "epoch [12/100], Loss: inf\n",
      "epoch [13/100], Loss: inf\n",
      "epoch [14/100], Loss: inf\n",
      "epoch [15/100], Loss: inf\n",
      "epoch [16/100], Loss: inf\n",
      "epoch [17/100], Loss: inf\n",
      "epoch [18/100], Loss: inf\n",
      "epoch [19/100], Loss: nan\n",
      "epoch [20/100], Loss: nan\n",
      "epoch [21/100], Loss: nan\n",
      "epoch [22/100], Loss: nan\n",
      "epoch [23/100], Loss: nan\n",
      "epoch [24/100], Loss: nan\n",
      "epoch [25/100], Loss: nan\n",
      "epoch [26/100], Loss: nan\n",
      "epoch [27/100], Loss: nan\n",
      "epoch [28/100], Loss: nan\n",
      "epoch [29/100], Loss: nan\n",
      "epoch [30/100], Loss: nan\n",
      "epoch [31/100], Loss: nan\n",
      "epoch [32/100], Loss: nan\n",
      "epoch [33/100], Loss: nan\n",
      "epoch [34/100], Loss: nan\n",
      "epoch [35/100], Loss: nan\n",
      "epoch [36/100], Loss: nan\n",
      "epoch [37/100], Loss: nan\n",
      "epoch [38/100], Loss: nan\n",
      "epoch [39/100], Loss: nan\n",
      "epoch [40/100], Loss: nan\n",
      "epoch [41/100], Loss: nan\n",
      "epoch [42/100], Loss: nan\n",
      "epoch [43/100], Loss: nan\n",
      "epoch [44/100], Loss: nan\n",
      "epoch [45/100], Loss: nan\n",
      "epoch [46/100], Loss: nan\n",
      "epoch [47/100], Loss: nan\n",
      "epoch [48/100], Loss: nan\n",
      "epoch [49/100], Loss: nan\n",
      "epoch [50/100], Loss: nan\n",
      "epoch [51/100], Loss: nan\n",
      "epoch [52/100], Loss: nan\n",
      "epoch [53/100], Loss: nan\n",
      "epoch [54/100], Loss: nan\n",
      "epoch [55/100], Loss: nan\n",
      "epoch [56/100], Loss: nan\n",
      "epoch [57/100], Loss: nan\n",
      "epoch [58/100], Loss: nan\n",
      "epoch [59/100], Loss: nan\n",
      "epoch [60/100], Loss: nan\n",
      "epoch [61/100], Loss: nan\n",
      "epoch [62/100], Loss: nan\n",
      "epoch [63/100], Loss: nan\n",
      "epoch [64/100], Loss: nan\n",
      "epoch [65/100], Loss: nan\n",
      "epoch [66/100], Loss: nan\n",
      "epoch [67/100], Loss: nan\n",
      "epoch [68/100], Loss: nan\n",
      "epoch [69/100], Loss: nan\n",
      "epoch [70/100], Loss: nan\n",
      "epoch [71/100], Loss: nan\n",
      "epoch [72/100], Loss: nan\n",
      "epoch [73/100], Loss: nan\n",
      "epoch [74/100], Loss: nan\n",
      "epoch [75/100], Loss: nan\n",
      "epoch [76/100], Loss: nan\n",
      "epoch [77/100], Loss: nan\n",
      "epoch [78/100], Loss: nan\n",
      "epoch [79/100], Loss: nan\n",
      "epoch [80/100], Loss: nan\n",
      "epoch [81/100], Loss: nan\n",
      "epoch [82/100], Loss: nan\n",
      "epoch [83/100], Loss: nan\n",
      "epoch [84/100], Loss: nan\n",
      "epoch [85/100], Loss: nan\n",
      "epoch [86/100], Loss: nan\n",
      "epoch [87/100], Loss: nan\n",
      "epoch [88/100], Loss: nan\n",
      "epoch [89/100], Loss: nan\n",
      "epoch [90/100], Loss: nan\n",
      "epoch [91/100], Loss: nan\n",
      "epoch [92/100], Loss: nan\n",
      "epoch [93/100], Loss: nan\n",
      "epoch [94/100], Loss: nan\n",
      "epoch [95/100], Loss: nan\n",
      "epoch [96/100], Loss: nan\n",
      "epoch [97/100], Loss: nan\n",
      "epoch [98/100], Loss: nan\n",
      "epoch [99/100], Loss: nan\n",
      "epoch [100/100], Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import dbapi\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# AWS CREDENTIALS\n",
    "HOST = \"hedgedb.c288vca6ravj.us-east-2.rds.amazonaws.com\"\n",
    "PORT = 3306\n",
    "DB_NAME = \"scores_timeseries\"\n",
    "DB_USER = \"hedgeADMIN\"\n",
    "DB_PW = \"bluefootedboobie123\"\n",
    "\n",
    "# connect to Dataset and AWS to pull data \n",
    "scores_db = dataset.connect(\"sqlite:///scorebase.db\")\n",
    "AWS_RDS =  dataset.connect(\"mysql+pymysql://{}:{}@{}/{}\".format\\\n",
    "(DB_USER, DB_PW, HOST, DB_NAME), engine_kwargs = {'pool_recycle': 3600})\n",
    "\n",
    "\n",
    "in_size = 3 # twitter_sent, headline_sent, wiki_views\n",
    "out_size = 1 # composite output\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "\n",
    "#Data set\n",
    "\n",
    "#x_train = np.array([[1.564],[2.11],[3.3],[5.4]], dtype=np.float32)\n",
    "x_train = np.array([[450,400,400],[300,300,350],[340,333,400],[421,308,309],[350,440,400]],dtype=np.float32)\n",
    "\n",
    "#y_train = np.array([[8.0],[19.0],[25.0],[34.45]], dtype= np.float32)\n",
    "y_train = np.array([[320],[180],[20],[100],[50]],dtype=np.float32)\n",
    "\n",
    "print('x_train:\\n',x_train)\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) #Forward propogation using linear model\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(in_size,out_size)\n",
    "\n",
    "#Lost and Optimizer\n",
    "criterion = nn.MSELoss() # using Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) # using Stochastic Gradient Descent\n",
    "\n",
    "# train the Model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #convert numpy arrays for training and results to torch tensor Variable class\n",
    "    inputs = Variable(x_train)\n",
    "    target = Variable(y_train)\n",
    "\n",
    "    #forward, backward, optimize\n",
    "    outputs = model(inputs) # generate output from model with all input vectors\n",
    "    loss = criterion(outputs,target) #loss function\n",
    "    \n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() #backward propogation\n",
    "    optimizer.step() #1-step optimization(gradient descent)\n",
    "    \n",
    "    if(epoch+1) % 1 ==0:\n",
    "        print('epoch [%d/%d], Loss: %.4f' % (epoch +1, num_epochs, loss.data[0]))\n",
    "        \n",
    "       \n",
    "\n",
    "model.eval()\n",
    "predicted = model(Variable(x_train)).data.numpy()\n",
    "      \n",
    "plt.plot(x_train.numpy(), y_train.numpy(),'ro',label='Original Data')\n",
    "plt.plot(x_train.numpy(), predicted,label='Fitted Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      " [[  450.    80. 14752.]\n",
      " [  300.    88. 11000.]\n",
      " [  260.    91.  9000.]\n",
      " [  496.    98.  1000.]\n",
      " [  200.    63.  2000.]]\n",
      "y_train:\n",
      " [[3.2]\n",
      " [1.8]\n",
      " [0.2]\n",
      " [1. ]\n",
      " [0.5]]\n",
      "epoch [1/300], Loss: 233.4949\n",
      "epoch [2/300], Loss: 11191.9092\n",
      "epoch [3/300], Loss: 233.4949\n",
      "epoch [4/300], Loss: 11191.9092\n",
      "epoch [5/300], Loss: 233.4949\n",
      "epoch [6/300], Loss: 11191.9092\n",
      "epoch [7/300], Loss: 233.4949\n",
      "epoch [8/300], Loss: 11191.9092\n",
      "epoch [9/300], Loss: 233.4949\n",
      "epoch [10/300], Loss: 11191.9092\n",
      "epoch [11/300], Loss: 233.4949\n",
      "epoch [12/300], Loss: 11191.9092\n",
      "epoch [13/300], Loss: 233.4949\n",
      "epoch [14/300], Loss: 11191.9092\n",
      "epoch [15/300], Loss: 233.4949\n",
      "epoch [16/300], Loss: 11191.9092\n",
      "epoch [17/300], Loss: 233.4949\n",
      "epoch [18/300], Loss: 11191.9092\n",
      "epoch [19/300], Loss: 233.4949\n",
      "epoch [20/300], Loss: 11191.9092\n",
      "epoch [21/300], Loss: 233.4949\n",
      "epoch [22/300], Loss: 11191.9092\n",
      "epoch [23/300], Loss: 233.4949\n",
      "epoch [24/300], Loss: 11191.9092\n",
      "epoch [25/300], Loss: 233.4949\n",
      "epoch [26/300], Loss: 11191.9092\n",
      "epoch [27/300], Loss: 233.4949\n",
      "epoch [28/300], Loss: 11191.9092\n",
      "epoch [29/300], Loss: 233.4949\n",
      "epoch [30/300], Loss: 11191.9092\n",
      "epoch [31/300], Loss: 233.4949\n",
      "epoch [32/300], Loss: 11191.9092\n",
      "epoch [33/300], Loss: 233.4949\n",
      "epoch [34/300], Loss: 11191.9092\n",
      "epoch [35/300], Loss: 233.4949\n",
      "epoch [36/300], Loss: 11191.9092\n",
      "epoch [37/300], Loss: 233.4949\n",
      "epoch [38/300], Loss: 11191.9092\n",
      "epoch [39/300], Loss: 233.4949\n",
      "epoch [40/300], Loss: 11191.9092\n",
      "epoch [41/300], Loss: 233.4949\n",
      "epoch [42/300], Loss: 11191.9092\n",
      "epoch [43/300], Loss: 233.4949\n",
      "epoch [44/300], Loss: 11191.9092\n",
      "epoch [45/300], Loss: 233.4949\n",
      "epoch [46/300], Loss: 11191.9092\n",
      "epoch [47/300], Loss: 233.4949\n",
      "epoch [48/300], Loss: 11191.9092\n",
      "epoch [49/300], Loss: 233.4949\n",
      "epoch [50/300], Loss: 11191.9092\n",
      "epoch [51/300], Loss: 233.4949\n",
      "epoch [52/300], Loss: 11191.9092\n",
      "epoch [53/300], Loss: 233.4949\n",
      "epoch [54/300], Loss: 11191.9092\n",
      "epoch [55/300], Loss: 233.4949\n",
      "epoch [56/300], Loss: 11191.9092\n",
      "epoch [57/300], Loss: 233.4949\n",
      "epoch [58/300], Loss: 11191.9092\n",
      "epoch [59/300], Loss: 233.4949\n",
      "epoch [60/300], Loss: 11191.9092\n",
      "epoch [61/300], Loss: 233.4949\n",
      "epoch [62/300], Loss: 11191.9092\n",
      "epoch [63/300], Loss: 233.4949\n",
      "epoch [64/300], Loss: 11191.9092\n",
      "epoch [65/300], Loss: 233.4949\n",
      "epoch [66/300], Loss: 11191.9092\n",
      "epoch [67/300], Loss: 233.4949\n",
      "epoch [68/300], Loss: 11191.9092\n",
      "epoch [69/300], Loss: 233.4949\n",
      "epoch [70/300], Loss: 11191.9092\n",
      "epoch [71/300], Loss: 233.4949\n",
      "epoch [72/300], Loss: 11191.9092\n",
      "epoch [73/300], Loss: 233.4949\n",
      "epoch [74/300], Loss: 11191.9092\n",
      "epoch [75/300], Loss: 233.4949\n",
      "epoch [76/300], Loss: 11191.9092\n",
      "epoch [77/300], Loss: 233.4949\n",
      "epoch [78/300], Loss: 11191.9092\n",
      "epoch [79/300], Loss: 233.4949\n",
      "epoch [80/300], Loss: 11191.9092\n",
      "epoch [81/300], Loss: 233.4949\n",
      "epoch [82/300], Loss: 11191.9092\n",
      "epoch [83/300], Loss: 233.4949\n",
      "epoch [84/300], Loss: 11191.9092\n",
      "epoch [85/300], Loss: 233.4949\n",
      "epoch [86/300], Loss: 11191.9092\n",
      "epoch [87/300], Loss: 233.4949\n",
      "epoch [88/300], Loss: 11191.9092\n",
      "epoch [89/300], Loss: 233.4949\n",
      "epoch [90/300], Loss: 11191.9092\n",
      "epoch [91/300], Loss: 233.4949\n",
      "epoch [92/300], Loss: 11191.9092\n",
      "epoch [93/300], Loss: 233.4949\n",
      "epoch [94/300], Loss: 11191.9092\n",
      "epoch [95/300], Loss: 233.4949\n",
      "epoch [96/300], Loss: 11191.9092\n",
      "epoch [97/300], Loss: 233.4949\n",
      "epoch [98/300], Loss: 11191.9092\n",
      "epoch [99/300], Loss: 233.4949\n",
      "epoch [100/300], Loss: 11191.9092\n",
      "epoch [101/300], Loss: 233.4949\n",
      "epoch [102/300], Loss: 11191.9092\n",
      "epoch [103/300], Loss: 233.4949\n",
      "epoch [104/300], Loss: 11191.9092\n",
      "epoch [105/300], Loss: 233.4949\n",
      "epoch [106/300], Loss: 11191.9092\n",
      "epoch [107/300], Loss: 233.4949\n",
      "epoch [108/300], Loss: 11191.9092\n",
      "epoch [109/300], Loss: 233.4949\n",
      "epoch [110/300], Loss: 11191.9092\n",
      "epoch [111/300], Loss: 233.4949\n",
      "epoch [112/300], Loss: 11191.9092\n",
      "epoch [113/300], Loss: 233.4949\n",
      "epoch [114/300], Loss: 11191.9092\n",
      "epoch [115/300], Loss: 233.4949\n",
      "epoch [116/300], Loss: 11191.9092\n",
      "epoch [117/300], Loss: 233.4949\n",
      "epoch [118/300], Loss: 11191.9092\n",
      "epoch [119/300], Loss: 233.4949\n",
      "epoch [120/300], Loss: 11191.9092\n",
      "epoch [121/300], Loss: 233.4949\n",
      "epoch [122/300], Loss: 11191.9092\n",
      "epoch [123/300], Loss: 233.4949\n",
      "epoch [124/300], Loss: 11191.9092\n",
      "epoch [125/300], Loss: 233.4949\n",
      "epoch [126/300], Loss: 11191.9092\n",
      "epoch [127/300], Loss: 233.4949\n",
      "epoch [128/300], Loss: 11191.9092\n",
      "epoch [129/300], Loss: 233.4949\n",
      "epoch [130/300], Loss: 11191.9092\n",
      "epoch [131/300], Loss: 233.4949\n",
      "epoch [132/300], Loss: 11191.9092\n",
      "epoch [133/300], Loss: 233.4949\n",
      "epoch [134/300], Loss: 11191.9092\n",
      "epoch [135/300], Loss: 233.4949\n",
      "epoch [136/300], Loss: 11191.9092\n",
      "epoch [137/300], Loss: 233.4949\n",
      "epoch [138/300], Loss: 11191.9092\n",
      "epoch [139/300], Loss: 233.4949\n",
      "epoch [140/300], Loss: 11191.9092\n",
      "epoch [141/300], Loss: 233.4949\n",
      "epoch [142/300], Loss: 11191.9092\n",
      "epoch [143/300], Loss: 233.4949\n",
      "epoch [144/300], Loss: 11191.9092\n",
      "epoch [145/300], Loss: 233.4949\n",
      "epoch [146/300], Loss: 11191.9092\n",
      "epoch [147/300], Loss: 233.4949\n",
      "epoch [148/300], Loss: 11191.9092\n",
      "epoch [149/300], Loss: 233.4949\n",
      "epoch [150/300], Loss: 11191.9092\n",
      "epoch [151/300], Loss: 233.4949\n",
      "epoch [152/300], Loss: 11191.9092\n",
      "epoch [153/300], Loss: 233.4949\n",
      "epoch [154/300], Loss: 11191.9092\n",
      "epoch [155/300], Loss: 233.4949\n",
      "epoch [156/300], Loss: 11191.9092\n",
      "epoch [157/300], Loss: 233.4949\n",
      "epoch [158/300], Loss: 11191.9092\n",
      "epoch [159/300], Loss: 233.4949\n",
      "epoch [160/300], Loss: 11191.9092\n",
      "epoch [161/300], Loss: 233.4949\n",
      "epoch [162/300], Loss: 11191.9092\n",
      "epoch [163/300], Loss: 233.4949\n",
      "epoch [164/300], Loss: 11191.9092\n",
      "epoch [165/300], Loss: 233.4949\n",
      "epoch [166/300], Loss: 11191.9092\n",
      "epoch [167/300], Loss: 233.4949\n",
      "epoch [168/300], Loss: 11191.9092\n",
      "epoch [169/300], Loss: 233.4949\n",
      "epoch [170/300], Loss: 11191.9092\n",
      "epoch [171/300], Loss: 233.4949\n",
      "epoch [172/300], Loss: 11191.9092\n",
      "epoch [173/300], Loss: 233.4949\n",
      "epoch [174/300], Loss: 11191.9092\n",
      "epoch [175/300], Loss: 233.4949\n",
      "epoch [176/300], Loss: 11191.9092\n",
      "epoch [177/300], Loss: 233.4949\n",
      "epoch [178/300], Loss: 11191.9092\n",
      "epoch [179/300], Loss: 233.4949\n",
      "epoch [180/300], Loss: 11191.9092\n",
      "epoch [181/300], Loss: 233.4949\n",
      "epoch [182/300], Loss: 11191.9092\n",
      "epoch [183/300], Loss: 233.4949\n",
      "epoch [184/300], Loss: 11191.9092\n",
      "epoch [185/300], Loss: 233.4949\n",
      "epoch [186/300], Loss: 11191.9092\n",
      "epoch [187/300], Loss: 233.4949\n",
      "epoch [188/300], Loss: 11191.9092\n",
      "epoch [189/300], Loss: 233.4949\n",
      "epoch [190/300], Loss: 11191.9092\n",
      "epoch [191/300], Loss: 233.4949\n",
      "epoch [192/300], Loss: 11191.9092\n",
      "epoch [193/300], Loss: 233.4949\n",
      "epoch [194/300], Loss: 11191.9092\n",
      "epoch [195/300], Loss: 233.4949\n",
      "epoch [196/300], Loss: 11191.9092\n",
      "epoch [197/300], Loss: 233.4949\n",
      "epoch [198/300], Loss: 11191.9092\n",
      "epoch [199/300], Loss: 233.4949\n",
      "epoch [200/300], Loss: 11191.9092\n",
      "epoch [201/300], Loss: 233.4949\n",
      "epoch [202/300], Loss: 11191.9092\n",
      "epoch [203/300], Loss: 233.4949\n",
      "epoch [204/300], Loss: 11191.9092\n",
      "epoch [205/300], Loss: 233.4949\n",
      "epoch [206/300], Loss: 11191.9092\n",
      "epoch [207/300], Loss: 233.4949\n",
      "epoch [208/300], Loss: 11191.9092\n",
      "epoch [209/300], Loss: 233.4949\n",
      "epoch [210/300], Loss: 11191.9092\n",
      "epoch [211/300], Loss: 233.4949\n",
      "epoch [212/300], Loss: 11191.9092\n",
      "epoch [213/300], Loss: 233.4949\n",
      "epoch [214/300], Loss: 11191.9092\n",
      "epoch [215/300], Loss: 233.4949\n",
      "epoch [216/300], Loss: 11191.9092\n",
      "epoch [217/300], Loss: 233.4949\n",
      "epoch [218/300], Loss: 11191.9092\n",
      "epoch [219/300], Loss: 233.4949\n",
      "epoch [220/300], Loss: 11191.9092\n",
      "epoch [221/300], Loss: 233.4949\n",
      "epoch [222/300], Loss: 11191.9092\n",
      "epoch [223/300], Loss: 233.4949\n",
      "epoch [224/300], Loss: 11191.9092\n",
      "epoch [225/300], Loss: 233.4949\n",
      "epoch [226/300], Loss: 11191.9092\n",
      "epoch [227/300], Loss: 233.4949\n",
      "epoch [228/300], Loss: 11191.9092\n",
      "epoch [229/300], Loss: 233.4949\n",
      "epoch [230/300], Loss: 11191.9092\n",
      "epoch [231/300], Loss: 233.4949\n",
      "epoch [232/300], Loss: 11191.9092\n",
      "epoch [233/300], Loss: 233.4949\n",
      "epoch [234/300], Loss: 11191.9092\n",
      "epoch [235/300], Loss: 233.4949\n",
      "epoch [236/300], Loss: 11191.9092\n",
      "epoch [237/300], Loss: 233.4949\n",
      "epoch [238/300], Loss: 11191.9092\n",
      "epoch [239/300], Loss: 233.4949\n",
      "epoch [240/300], Loss: 11191.9092\n",
      "epoch [241/300], Loss: 233.4949\n",
      "epoch [242/300], Loss: 11191.9092\n",
      "epoch [243/300], Loss: 233.4949\n",
      "epoch [244/300], Loss: 11191.9092\n",
      "epoch [245/300], Loss: 233.4949\n",
      "epoch [246/300], Loss: 11191.9092\n",
      "epoch [247/300], Loss: 233.4949\n",
      "epoch [248/300], Loss: 11191.9092\n",
      "epoch [249/300], Loss: 233.4949\n",
      "epoch [250/300], Loss: 11191.9092\n",
      "epoch [251/300], Loss: 233.4949\n",
      "epoch [252/300], Loss: 11191.9092\n",
      "epoch [253/300], Loss: 233.4949\n",
      "epoch [254/300], Loss: 11191.9092\n",
      "epoch [255/300], Loss: 233.4949\n",
      "epoch [256/300], Loss: 11191.9092\n",
      "epoch [257/300], Loss: 233.4949\n",
      "epoch [258/300], Loss: 11191.9092\n",
      "epoch [259/300], Loss: 233.4949\n",
      "epoch [260/300], Loss: 11191.9092\n",
      "epoch [261/300], Loss: 233.4949\n",
      "epoch [262/300], Loss: 11191.9092\n",
      "epoch [263/300], Loss: 233.4949\n",
      "epoch [264/300], Loss: 11191.9092\n",
      "epoch [265/300], Loss: 233.4949\n",
      "epoch [266/300], Loss: 11191.9092\n",
      "epoch [267/300], Loss: 233.4949\n",
      "epoch [268/300], Loss: 11191.9092\n",
      "epoch [269/300], Loss: 233.4949\n",
      "epoch [270/300], Loss: 11191.9092\n",
      "epoch [271/300], Loss: 233.4949\n",
      "epoch [272/300], Loss: 11191.9092\n",
      "epoch [273/300], Loss: 233.4949\n",
      "epoch [274/300], Loss: 11191.9092\n",
      "epoch [275/300], Loss: 233.4949\n",
      "epoch [276/300], Loss: 11191.9092\n",
      "epoch [277/300], Loss: 233.4949\n",
      "epoch [278/300], Loss: 11191.9092\n",
      "epoch [279/300], Loss: 233.4949\n",
      "epoch [280/300], Loss: 11191.9092\n",
      "epoch [281/300], Loss: 233.4949\n",
      "epoch [282/300], Loss: 11191.9092\n",
      "epoch [283/300], Loss: 233.4949\n",
      "epoch [284/300], Loss: 11191.9092\n",
      "epoch [285/300], Loss: 233.4949\n",
      "epoch [286/300], Loss: 11191.9092\n",
      "epoch [287/300], Loss: 233.4949\n",
      "epoch [288/300], Loss: 11191.9092\n",
      "epoch [289/300], Loss: 233.4949\n",
      "epoch [290/300], Loss: 11191.9092\n",
      "epoch [291/300], Loss: 233.4949\n",
      "epoch [292/300], Loss: 11191.9092\n",
      "epoch [293/300], Loss: 233.4949\n",
      "epoch [294/300], Loss: 11191.9092\n",
      "epoch [295/300], Loss: 233.4949\n",
      "epoch [296/300], Loss: 11191.9092\n",
      "epoch [297/300], Loss: 233.4949\n",
      "epoch [298/300], Loss: 11191.9092\n",
      "epoch [299/300], Loss: 233.4949\n",
      "epoch [300/300], Loss: 11191.9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:80: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import dataset\n",
    "import dbapi\n",
    "import sqlalchemy\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# AWS CREDENTIALS\n",
    "HOST = \"hedgedb.c288vca6ravj.us-east-2.rds.amazonaws.com\"\n",
    "PORT = 3306\n",
    "DB_NAME = \"scores_timeseries\"\n",
    "DB_USER = \"hedgeADMIN\"\n",
    "DB_PW = \"bluefootedboobie123\"\n",
    "\n",
    "# connect to Dataset and AWS to pull data \n",
    "scores_db = dataset.connect(\"sqlite:///scorebase.db\")\n",
    "AWS_RDS =  dataset.connect(\"mysql+pymysql://{}:{}@{}/{}\".format\\\n",
    "(DB_USER, DB_PW, HOST, DB_NAME), engine_kwargs = {'pool_recycle': 3600})\n",
    "\n",
    "\n",
    "in_size = 3 # twitter_sent, headline_sent, wiki_views\n",
    "out_size = 1 # composite output\n",
    "num_epochs = 300\n",
    "learning_rate = 0.02\n",
    "\n",
    "\n",
    "#Data set\n",
    "\n",
    "#x_train = np.array([[1.564],[2.11],[3.3],[5.4]], dtype=np.float32)\n",
    "x_train = np.array([[450.,80.,14752.],[300.,88.,11000.],[260.,91.,9000.],[496.,98.,1000.],[200.,63.,2000.]],dtype=np.float32)\n",
    "\n",
    "#y_train = np.array([[8.0],[19.0],[25.0],[34.45]], dtype= np.float32)\n",
    "y_train = np.array([[3.2],[1.8],[0.2],[1.0],[0.5]],dtype=np.float32)\n",
    "\n",
    "print('x_train:\\n',x_train)\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) #Forward propogation using linear model\n",
    "        return out\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "#Lost and Optimizer\n",
    "criterion = nn.SmoothL1Loss() # using Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) # using Stochastic Gradient Descent\n",
    "\n",
    "# train the Model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #convert numpy arrays for training and results to torch tensor Variable class\n",
    "    inputs = Variable(x_train)\n",
    "    target = Variable(y_train)\n",
    "\n",
    "    #forward\n",
    "    outputs = model(inputs) # generate output from model with all input vectors\n",
    "    loss = criterion(outputs,target) #loss function\n",
    "    \n",
    "    #backwards\n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    loss.backward() #backward propogation\n",
    "    optimizer.step() #1-step optimization(gradient descent)\n",
    "    \n",
    "    if(epoch+1) % 1 ==0:\n",
    "        print('epoch [%d/%d], Loss: %.4f' % (epoch +1, num_epochs, loss.data[0]))\n",
    "        \n",
    "       \n",
    "model.eval()\n",
    "predicted = model(Variable(x_train)).data.numpy()\n",
    "      \n",
    "plt.plot(x_train.numpy(), y_train.numpy(),'ro',label='Original Data')\n",
    "plt.plot(x_train.numpy(), predicted,label='Fitted Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
